{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed. The merged dataset is saved as 'New_Merged_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the data\n",
    "sentinel_data_path = 'Sentinel2_Data.csv'\n",
    "ground_data_path = 'Ground_Data.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "sentinel_df = pd.read_csv(sentinel_data_path, parse_dates=['timestamp'])\n",
    "ground_df = pd.read_csv(ground_data_path, parse_dates=['Timestamp (UTC+12:00)'], low_memory=False)\n",
    "\n",
    "# Rename timestamp columns\n",
    "sentinel_df.rename(columns={'timestamp': 'timestamp_sentinel2'}, inplace=True)\n",
    "ground_df.rename(columns={'Timestamp (UTC+12:00)': 'timestamp_ground'}, inplace=True)\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "numeric_cols = [\n",
    "    'Discharge_(m^3/s)', 'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)',\n",
    "    'Snow_Volume_Opuha_Catchment_(mm)', 'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)',\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)', 'WDir(Deg)', 'WSpd(m/s)',\n",
    "    'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)', 'Tdry(C)', 'TWet(C)',\n",
    "    'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    ground_df[col] = pd.to_numeric(ground_df[col], errors='coerce')\n",
    "\n",
    "# Separate ground data based on sampling intervals\n",
    "three_hour_avg_cols = ['Discharge_(m^3/s)']\n",
    "fifteen_min_avg_cols = [\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)',\n",
    "    'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)'\n",
    "]\n",
    "daily_avg_cols = [\n",
    "    'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)', 'Snow_Volume_Opuha_Catchment_(mm)',\n",
    "    'WDir(Deg)', 'WSpd(m/s)', 'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)',\n",
    "    'Tdry(C)', 'TWet(C)', 'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "# Function to find the closest ground measurement to a given Sentinel-2 timestamp and compute time difference\n",
    "def find_closest_measurement_time(sentinel_time, ground_df, cols, time_window):\n",
    "    start_time = sentinel_time - time_window\n",
    "    end_time = sentinel_time + time_window\n",
    "    filtered_ground_df = ground_df[(ground_df['timestamp_ground'] >= start_time) & (ground_df['timestamp_ground'] <= end_time)]\n",
    "    if not filtered_ground_df.empty:\n",
    "        closest_time = filtered_ground_df['timestamp_ground'].iloc[(filtered_ground_df['timestamp_ground'] - sentinel_time).abs().argsort()[:1]].iloc[0]\n",
    "        time_diff = abs((closest_time - sentinel_time).total_seconds())  # Time difference in seconds\n",
    "        closest_rows = filtered_ground_df[filtered_ground_df['timestamp_ground'] == closest_time][cols].iloc[0].to_dict()\n",
    "        return closest_rows, time_diff\n",
    "    else:\n",
    "        return {col: None for col in cols}, None\n",
    "\n",
    "# Define the time window for finer intervals (e.g., 1 hour)\n",
    "time_window = timedelta(hours=3)\n",
    "\n",
    "# Prepare a list to store merged data\n",
    "merged_data = []\n",
    "\n",
    "# Process each Sentinel-2 timestamp\n",
    "for index, row in sentinel_df.iterrows():\n",
    "    sentinel_time = row['timestamp_sentinel2']\n",
    "    merged_row = row.to_dict()\n",
    "    \n",
    "    # Find closest measurements and time difference for three hour average and fifteen minute interval columns\n",
    "    closest_three_hour, time_diff_three_hour = find_closest_measurement_time(sentinel_time, ground_df, three_hour_avg_cols, time_window)\n",
    "    closest_fifteen_min, time_diff_fifteen_min = find_closest_measurement_time(sentinel_time, ground_df, fifteen_min_avg_cols, time_window)\n",
    "    \n",
    "    # Update the merged row with these measurements\n",
    "    merged_row.update(closest_three_hour)\n",
    "    merged_row.update(closest_fifteen_min)\n",
    "    \n",
    "    # Add the time difference to the merged row (use the time difference from the finer interval measurements)\n",
    "    merged_row['Ground_Measurements_time_diff_(seconds)'] = time_diff_fifteen_min if time_diff_fifteen_min is not None else time_diff_three_hour\n",
    "    \n",
    "    merged_data.append(merged_row)\n",
    "\n",
    "# Convert the merged data to a DataFrame\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Create date columns for merging daily averages\n",
    "ground_df['date'] = ground_df['timestamp_ground'].dt.date\n",
    "sentinel_df['date'] = sentinel_df['timestamp_sentinel2'].dt.date\n",
    "merged_df['date'] = pd.to_datetime(merged_df['timestamp_sentinel2']).dt.date\n",
    "\n",
    "# Calculate daily averages for relevant ground data\n",
    "daily_avg_df = ground_df.groupby('date')[daily_avg_cols].mean().reset_index()\n",
    "\n",
    "# Merge the daily averages with the Sentinel-2 data\n",
    "merged_df = pd.merge(merged_df, daily_avg_df, on='date', how='left')\n",
    "\n",
    "# Drop the 'date' column as it was only for merging purposes\n",
    "merged_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('New_Merged_Data.csv', index=False)\n",
    "\n",
    "print(\"Merging completed. The merged dataset is saved as 'New_Merged_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation completed. The updated dataset is saved as 'Updated_Merged_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the merged data\n",
    "merged_data_path = 'New_Merged_Data.csv'\n",
    "merged_df = pd.read_csv(merged_data_path)\n",
    "\n",
    "# List of band prefixes to aggregate\n",
    "bands = ['B2', 'B3', 'B4', 'B8', 'B8A', 'B11', 'B12']\n",
    "\n",
    "# Compute averages for each band\n",
    "for band in bands:\n",
    "    band_cols = [col for col in merged_df.columns if f'_{band}' in col]\n",
    "    merged_df[f'{band}_AVG'] = merged_df[band_cols].mean(axis=1)\n",
    "\n",
    "# Compute averages for cs and cs_CDF\n",
    "cs_cols = [col for col in merged_df.columns if '_cs' in col and '_cs_cdf' not in col]\n",
    "cs_cdf_cols = [col for col in merged_df.columns if '_cs_cdf' in col]\n",
    "\n",
    "merged_df['cs_AVG'] = merged_df[cs_cols].mean(axis=1)\n",
    "merged_df['cs_cdf_AVG'] = merged_df[cs_cdf_cols].mean(axis=1)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "updated_data_path = 'Updated_Merged_Data.csv'\n",
    "merged_df.to_csv(updated_data_path, index=False)\n",
    "\n",
    "print(\"Aggregation completed. The updated dataset is saved as 'Updated_Merged_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_sentinel2</th>\n",
       "      <th>point1_B2</th>\n",
       "      <th>point10_B2</th>\n",
       "      <th>point11_B2</th>\n",
       "      <th>point12_B2</th>\n",
       "      <th>point13_B2</th>\n",
       "      <th>point14_B2</th>\n",
       "      <th>point15_B2</th>\n",
       "      <th>point16_B2</th>\n",
       "      <th>point17_B2</th>\n",
       "      <th>...</th>\n",
       "      <th>Pstn(hPa)</th>\n",
       "      <th>B2_AVG</th>\n",
       "      <th>B3_AVG</th>\n",
       "      <th>B4_AVG</th>\n",
       "      <th>B8_AVG</th>\n",
       "      <th>B8A_AVG</th>\n",
       "      <th>B11_AVG</th>\n",
       "      <th>B12_AVG</th>\n",
       "      <th>cs_AVG</th>\n",
       "      <th>cs_cdf_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-14 10:38:15</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>2138.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>2288.0</td>\n",
       "      <td>2678.0</td>\n",
       "      <td>2924.0</td>\n",
       "      <td>3312.0</td>\n",
       "      <td>2958.0</td>\n",
       "      <td>2958.0</td>\n",
       "      <td>...</td>\n",
       "      <td>972.1</td>\n",
       "      <td>2527.909091</td>\n",
       "      <td>2646.818182</td>\n",
       "      <td>2544.090909</td>\n",
       "      <td>3888.590909</td>\n",
       "      <td>4010.454545</td>\n",
       "      <td>3389.363636</td>\n",
       "      <td>3013.727273</td>\n",
       "      <td>0.185740</td>\n",
       "      <td>0.230125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-16 10:28:16</td>\n",
       "      <td>8680.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>8472.0</td>\n",
       "      <td>8424.0</td>\n",
       "      <td>8576.0</td>\n",
       "      <td>8504.0</td>\n",
       "      <td>8472.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>979.3</td>\n",
       "      <td>8394.545455</td>\n",
       "      <td>7825.454545</td>\n",
       "      <td>7425.636364</td>\n",
       "      <td>7479.363636</td>\n",
       "      <td>7411.090909</td>\n",
       "      <td>5478.409091</td>\n",
       "      <td>4051.136364</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.043494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-19 10:38:13</td>\n",
       "      <td>6748.0</td>\n",
       "      <td>6884.0</td>\n",
       "      <td>6860.0</td>\n",
       "      <td>6808.0</td>\n",
       "      <td>6784.0</td>\n",
       "      <td>6732.0</td>\n",
       "      <td>6808.0</td>\n",
       "      <td>6860.0</td>\n",
       "      <td>6860.0</td>\n",
       "      <td>...</td>\n",
       "      <td>961.9</td>\n",
       "      <td>6819.818182</td>\n",
       "      <td>6948.000000</td>\n",
       "      <td>7082.363636</td>\n",
       "      <td>7510.954545</td>\n",
       "      <td>7456.636364</td>\n",
       "      <td>3911.636364</td>\n",
       "      <td>2789.227273</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.057041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-21 10:28:20</td>\n",
       "      <td>2464.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>1282.0</td>\n",
       "      <td>2364.0</td>\n",
       "      <td>2460.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>953.9</td>\n",
       "      <td>1979.000000</td>\n",
       "      <td>2141.090909</td>\n",
       "      <td>2041.818182</td>\n",
       "      <td>3568.590909</td>\n",
       "      <td>3731.272727</td>\n",
       "      <td>2775.863636</td>\n",
       "      <td>2461.590909</td>\n",
       "      <td>0.065597</td>\n",
       "      <td>0.169875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-26 10:28:18</td>\n",
       "      <td>11912.0</td>\n",
       "      <td>11776.0</td>\n",
       "      <td>11920.0</td>\n",
       "      <td>11920.0</td>\n",
       "      <td>11944.0</td>\n",
       "      <td>11984.0</td>\n",
       "      <td>11984.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>965.0</td>\n",
       "      <td>11913.090909</td>\n",
       "      <td>10787.636364</td>\n",
       "      <td>10084.363636</td>\n",
       "      <td>9416.772727</td>\n",
       "      <td>9212.454545</td>\n",
       "      <td>5895.136364</td>\n",
       "      <td>4535.409091</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.043137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2024-06-15 10:38:25</td>\n",
       "      <td>467.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>318.818182</td>\n",
       "      <td>332.818182</td>\n",
       "      <td>333.454545</td>\n",
       "      <td>558.750000</td>\n",
       "      <td>611.227273</td>\n",
       "      <td>539.500000</td>\n",
       "      <td>467.363636</td>\n",
       "      <td>0.348128</td>\n",
       "      <td>0.626381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2024-06-17 10:28:31</td>\n",
       "      <td>2852.0</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>2908.0</td>\n",
       "      <td>2956.0</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>2862.0</td>\n",
       "      <td>2862.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2876.909091</td>\n",
       "      <td>2878.727273</td>\n",
       "      <td>2971.090909</td>\n",
       "      <td>3414.340909</td>\n",
       "      <td>3319.772727</td>\n",
       "      <td>1259.045455</td>\n",
       "      <td>1353.500000</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.090018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>2024-06-20 10:38:27</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302.545455</td>\n",
       "      <td>1328.545455</td>\n",
       "      <td>1301.181818</td>\n",
       "      <td>1449.863636</td>\n",
       "      <td>1415.181818</td>\n",
       "      <td>1078.409091</td>\n",
       "      <td>967.500000</td>\n",
       "      <td>0.349554</td>\n",
       "      <td>0.597326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>2024-06-22 10:28:27</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006.863636</td>\n",
       "      <td>1022.954545</td>\n",
       "      <td>953.272727</td>\n",
       "      <td>1134.159091</td>\n",
       "      <td>1239.272727</td>\n",
       "      <td>1001.545455</td>\n",
       "      <td>967.318182</td>\n",
       "      <td>0.512834</td>\n",
       "      <td>0.785918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>2024-06-25 10:38:25</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>2286.0</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2270.000000</td>\n",
       "      <td>2093.909091</td>\n",
       "      <td>1894.181818</td>\n",
       "      <td>2155.181818</td>\n",
       "      <td>2017.545455</td>\n",
       "      <td>1510.454545</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>0.329055</td>\n",
       "      <td>0.602496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>754 rows Ã— 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp_sentinel2  point1_B2  point10_B2  point11_B2  point12_B2  \\\n",
       "0    2018-12-14 10:38:15     2068.0      2138.0      2084.0      2288.0   \n",
       "1    2018-12-16 10:28:16     8680.0      8400.0      8472.0      8424.0   \n",
       "2    2018-12-19 10:38:13     6748.0      6884.0      6860.0      6808.0   \n",
       "3    2018-12-21 10:28:20     2464.0      1506.0      1236.0      1282.0   \n",
       "4    2018-12-26 10:28:18    11912.0     11776.0     11920.0     11920.0   \n",
       "..                   ...        ...         ...         ...         ...   \n",
       "749  2024-06-15 10:38:25      467.0       230.0       264.0       287.0   \n",
       "750  2024-06-17 10:28:31     2852.0      2836.0      2908.0      2956.0   \n",
       "751  2024-06-20 10:38:27     1416.0      1222.0      1372.0      1396.0   \n",
       "752  2024-06-22 10:28:27     1026.0       966.0      1094.0      1060.0   \n",
       "753  2024-06-25 10:38:25     2344.0      2130.0      2130.0      2150.0   \n",
       "\n",
       "     point13_B2  point14_B2  point15_B2  point16_B2  point17_B2  ...  \\\n",
       "0        2678.0      2924.0      3312.0      2958.0      2958.0  ...   \n",
       "1        8576.0      8504.0      8472.0      8400.0      8400.0  ...   \n",
       "2        6784.0      6732.0      6808.0      6860.0      6860.0  ...   \n",
       "3        2364.0      2460.0      2360.0      2216.0      2216.0  ...   \n",
       "4       11944.0     11984.0     11984.0     12080.0     12080.0  ...   \n",
       "..          ...         ...         ...         ...         ...  ...   \n",
       "749       433.0       364.0       360.0       350.0       350.0  ...   \n",
       "750      2880.0      2862.0      2862.0      2886.0      2886.0  ...   \n",
       "751      1316.0      1360.0      1312.0      1260.0      1260.0  ...   \n",
       "752      1086.0      1072.0      1034.0       966.0       966.0  ...   \n",
       "753      2332.0      2286.0      2344.0      2360.0      2360.0  ...   \n",
       "\n",
       "     Pstn(hPa)        B2_AVG        B3_AVG        B4_AVG       B8_AVG  \\\n",
       "0        972.1   2527.909091   2646.818182   2544.090909  3888.590909   \n",
       "1        979.3   8394.545455   7825.454545   7425.636364  7479.363636   \n",
       "2        961.9   6819.818182   6948.000000   7082.363636  7510.954545   \n",
       "3        953.9   1979.000000   2141.090909   2041.818182  3568.590909   \n",
       "4        965.0  11913.090909  10787.636364  10084.363636  9416.772727   \n",
       "..         ...           ...           ...           ...          ...   \n",
       "749        NaN    318.818182    332.818182    333.454545   558.750000   \n",
       "750        NaN   2876.909091   2878.727273   2971.090909  3414.340909   \n",
       "751        NaN   1302.545455   1328.545455   1301.181818  1449.863636   \n",
       "752        NaN   1006.863636   1022.954545    953.272727  1134.159091   \n",
       "753        NaN   2270.000000   2093.909091   1894.181818  2155.181818   \n",
       "\n",
       "         B8A_AVG      B11_AVG      B12_AVG    cs_AVG  cs_cdf_AVG  \n",
       "0    4010.454545  3389.363636  3013.727273  0.185740    0.230125  \n",
       "1    7411.090909  5478.409091  4051.136364  0.003922    0.043494  \n",
       "2    7456.636364  3911.636364  2789.227273  0.011765    0.057041  \n",
       "3    3731.272727  2775.863636  2461.590909  0.065597    0.169875  \n",
       "4    9212.454545  5895.136364  4535.409091  0.003922    0.043137  \n",
       "..           ...          ...          ...       ...         ...  \n",
       "749   611.227273   539.500000   467.363636  0.348128    0.626381  \n",
       "750  3319.772727  1259.045455  1353.500000  0.019964    0.090018  \n",
       "751  1415.181818  1078.409091   967.500000  0.349554    0.597326  \n",
       "752  1239.272727  1001.545455   967.318182  0.512834    0.785918  \n",
       "753  2017.545455  1510.454545  1348.000000  0.329055    0.602496  \n",
       "\n",
       "[754 rows x 232 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix up for the discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed. The merged dataset is saved as 'Updated_Merged_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the data\n",
    "sentinel_data_path = 'Sentinel2_Data.csv'\n",
    "ground_data_path = 'Ground_Data.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "sentinel_df = pd.read_csv(sentinel_data_path, parse_dates=['timestamp'])\n",
    "ground_df = pd.read_csv(ground_data_path, parse_dates=['Timestamp (UTC+12:00)'], low_memory=False)\n",
    "\n",
    "# Rename timestamp columns\n",
    "sentinel_df.rename(columns={'timestamp': 'timestamp_sentinel2'}, inplace=True)\n",
    "ground_df.rename(columns={'Timestamp (UTC+12:00)': 'timestamp_ground'}, inplace=True)\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "numeric_cols = [\n",
    "    'Discharge_(m^3/s)', 'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)',\n",
    "    'Snow_Volume_Opuha_Catchment_(mm)', 'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)',\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)', 'WDir(Deg)', 'WSpd(m/s)',\n",
    "    'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)', 'Tdry(C)', 'TWet(C)',\n",
    "    'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    ground_df[col] = pd.to_numeric(ground_df[col], errors='coerce')\n",
    "\n",
    "# Separate ground data based on sampling intervals\n",
    "fifteen_min_avg_cols = [\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)',\n",
    "    'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)'\n",
    "]\n",
    "daily_avg_cols = [\n",
    "    'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)', 'Snow_Volume_Opuha_Catchment_(mm)',\n",
    "    'WDir(Deg)', 'WSpd(m/s)', 'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)',\n",
    "    'Tdry(C)', 'TWet(C)', 'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "# Function to find the closest ground measurement to a given Sentinel-2 timestamp and compute time difference\n",
    "def find_closest_measurement_time(sentinel_time, ground_df, cols, time_window):\n",
    "    start_time = sentinel_time - time_window\n",
    "    end_time = sentinel_time + time_window\n",
    "    filtered_ground_df = ground_df[(ground_df['timestamp_ground'] >= start_time) & (ground_df['timestamp_ground'] <= end_time)]\n",
    "    if not filtered_ground_df.empty:\n",
    "        closest_time = filtered_ground_df['timestamp_ground'].iloc[(filtered_ground_df['timestamp_ground'] - sentinel_time).abs().argsort()[:1]].iloc[0]\n",
    "        time_diff = abs((closest_time - sentinel_time).total_seconds())  # Time difference in seconds\n",
    "        closest_rows = filtered_ground_df[filtered_ground_df['timestamp_ground'] == closest_time][cols].iloc[0].to_dict()\n",
    "        return closest_rows, time_diff\n",
    "    else:\n",
    "        return {col: None for col in cols}, None\n",
    "\n",
    "# Define the time window for measurements\n",
    "time_window = timedelta(hours=1)\n",
    "discharge_time_window = timedelta(hours=12)\n",
    "\n",
    "# Prepare a list to store merged data\n",
    "merged_data = []\n",
    "\n",
    "# Process each Sentinel-2 timestamp\n",
    "for index, row in sentinel_df.iterrows():\n",
    "    sentinel_time = row['timestamp_sentinel2']\n",
    "    merged_row = row.to_dict()\n",
    "    \n",
    "    # Find closest measurements and time difference for fifteen minute interval columns (excluding discharge)\n",
    "    closest_fifteen_min, time_diff_fifteen_min = find_closest_measurement_time(sentinel_time, ground_df, fifteen_min_avg_cols, time_window)\n",
    "    \n",
    "    # Update the merged row with these measurements\n",
    "    merged_row.update(closest_fifteen_min)\n",
    "    \n",
    "    # Add the time difference to the merged row (use the time difference from the finer interval measurements)\n",
    "    merged_row['Ground_Measurements_time_diff_(seconds)'] = time_diff_fifteen_min\n",
    "    \n",
    "    # Add the merged row to the list\n",
    "    merged_data.append(merged_row)\n",
    "\n",
    "# Convert the merged data to a DataFrame\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Find and add the closest discharge measurement within the 12-hour window\n",
    "for index, row in merged_df.iterrows():\n",
    "    sentinel_time = row['timestamp_sentinel2']\n",
    "    closest_discharge, _ = find_closest_measurement_time(sentinel_time, ground_df, ['Discharge_(m^3/s)'], discharge_time_window)\n",
    "    merged_df.at[index, 'Discharge_(m^3/s)'] = closest_discharge['Discharge_(m^3/s)']\n",
    "\n",
    "# Create date columns for merging daily averages\n",
    "ground_df['date'] = ground_df['timestamp_ground'].dt.date\n",
    "sentinel_df['date'] = sentinel_df['timestamp_sentinel2'].dt.date\n",
    "merged_df['date'] = pd.to_datetime(merged_df['timestamp_sentinel2']).dt.date\n",
    "\n",
    "# Calculate daily averages for relevant ground data\n",
    "daily_avg_df = ground_df.groupby('date')[daily_avg_cols].mean().reset_index()\n",
    "\n",
    "# Merge the daily averages with the Sentinel-2 data\n",
    "merged_df = pd.merge(merged_df, daily_avg_df, on='date', how='left')\n",
    "\n",
    "# Drop the 'date' column as it was only for merging purposes\n",
    "merged_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('Updated_Merged_Data_27_3.csv', index=False)\n",
    "\n",
    "print(\"Merging completed. The merged dataset is saved as 'Updated_Merged_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
