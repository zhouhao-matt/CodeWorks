{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed. The merged dataset is saved as 'New_Merged_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the data\n",
    "sentinel_data_path = 'Sentinel2_Data.csv'\n",
    "ground_data_path = 'Ground_Data.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "sentinel_df = pd.read_csv(sentinel_data_path, parse_dates=['timestamp'])\n",
    "ground_df = pd.read_csv(ground_data_path, parse_dates=['Timestamp (UTC+12:00)'], low_memory=False)\n",
    "\n",
    "# Rename timestamp columns\n",
    "sentinel_df.rename(columns={'timestamp': 'timestamp_sentinel2'}, inplace=True)\n",
    "ground_df.rename(columns={'Timestamp (UTC+12:00)': 'timestamp_ground'}, inplace=True)\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "numeric_cols = [\n",
    "    'Discharge_(m^3/s)', 'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)',\n",
    "    'Snow_Volume_Opuha_Catchment_(mm)', 'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)',\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)', 'WDir(Deg)', 'WSpd(m/s)',\n",
    "    'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)', 'Tdry(C)', 'TWet(C)',\n",
    "    'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    ground_df[col] = pd.to_numeric(ground_df[col], errors='coerce')\n",
    "\n",
    "# Separate ground data based on sampling intervals\n",
    "three_hour_avg_cols = ['Discharge_(m^3/s)']\n",
    "fifteen_min_avg_cols = [\n",
    "    'Water_Temp_Buoy_(degC)', 'Water_Temp_Platform_(degC)',\n",
    "    'Turbidity_Buoy_(NTU)', 'Turbidity_Platform_(NTU)'\n",
    "]\n",
    "daily_avg_cols = [\n",
    "    'Lake_Height_(m)', 'PercentFull_Active_Lake_Storage_(%)', 'Snow_Volume_Opuha_Catchment_(mm)',\n",
    "    'WDir(Deg)', 'WSpd(m/s)', 'GustDir(Deg)', 'GustSpd(m/s)', 'WindRun(Km)', 'Rain(mm)',\n",
    "    'Tdry(C)', 'TWet(C)', 'RH(%)', 'Tmax(C)', 'Tmin(C)', 'Pmsl(hPa)', 'Pstn(hPa)'\n",
    "]\n",
    "\n",
    "# Function to find the closest ground measurement to a given Sentinel-2 timestamp and compute time difference\n",
    "def find_closest_measurement_time(sentinel_time, ground_df, cols, time_window):\n",
    "    start_time = sentinel_time - time_window\n",
    "    end_time = sentinel_time + time_window\n",
    "    filtered_ground_df = ground_df[(ground_df['timestamp_ground'] >= start_time) & (ground_df['timestamp_ground'] <= end_time)]\n",
    "    if not filtered_ground_df.empty:\n",
    "        closest_time = filtered_ground_df['timestamp_ground'].iloc[(filtered_ground_df['timestamp_ground'] - sentinel_time).abs().argsort()[:1]].iloc[0]\n",
    "        time_diff = abs((closest_time - sentinel_time).total_seconds())  # Time difference in seconds\n",
    "        closest_rows = filtered_ground_df[filtered_ground_df['timestamp_ground'] == closest_time][cols].iloc[0].to_dict()\n",
    "        return closest_rows, time_diff\n",
    "    else:\n",
    "        return {col: None for col in cols}, None\n",
    "\n",
    "# Define the time window for finer intervals (e.g., 1 hour)\n",
    "time_window = timedelta(hours=1)\n",
    "\n",
    "# Prepare a list to store merged data\n",
    "merged_data = []\n",
    "\n",
    "# Process each Sentinel-2 timestamp\n",
    "for index, row in sentinel_df.iterrows():\n",
    "    sentinel_time = row['timestamp_sentinel2']\n",
    "    merged_row = row.to_dict()\n",
    "    \n",
    "    # Find closest measurements and time difference for three hour average and fifteen minute interval columns\n",
    "    closest_three_hour, time_diff_three_hour = find_closest_measurement_time(sentinel_time, ground_df, three_hour_avg_cols, time_window)\n",
    "    closest_fifteen_min, time_diff_fifteen_min = find_closest_measurement_time(sentinel_time, ground_df, fifteen_min_avg_cols, time_window)\n",
    "    \n",
    "    # Update the merged row with these measurements\n",
    "    merged_row.update(closest_three_hour)\n",
    "    merged_row.update(closest_fifteen_min)\n",
    "    \n",
    "    # Add the time difference to the merged row (use the time difference from the finer interval measurements)\n",
    "    merged_row['Ground_Measurements_time_diff_(seconds)'] = time_diff_fifteen_min if time_diff_fifteen_min is not None else time_diff_three_hour\n",
    "    \n",
    "    merged_data.append(merged_row)\n",
    "\n",
    "# Convert the merged data to a DataFrame\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Create date columns for merging daily averages\n",
    "ground_df['date'] = ground_df['timestamp_ground'].dt.date\n",
    "sentinel_df['date'] = sentinel_df['timestamp_sentinel2'].dt.date\n",
    "merged_df['date'] = pd.to_datetime(merged_df['timestamp_sentinel2']).dt.date\n",
    "\n",
    "# Calculate daily averages for relevant ground data\n",
    "daily_avg_df = ground_df.groupby('date')[daily_avg_cols].mean().reset_index()\n",
    "\n",
    "# Merge the daily averages with the Sentinel-2 data\n",
    "merged_df = pd.merge(merged_df, daily_avg_df, on='date', how='left')\n",
    "\n",
    "# Drop the 'date' column as it was only for merging purposes\n",
    "merged_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('New_Merged_Data.csv', index=False)\n",
    "\n",
    "print(\"Merging completed. The merged dataset is saved as 'New_Merged_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation completed. The updated dataset is saved as 'Updated_Merged_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the merged data\n",
    "merged_data_path = 'New_Merged_Data.csv'\n",
    "merged_df = pd.read_csv(merged_data_path)\n",
    "\n",
    "# List of band prefixes to aggregate\n",
    "bands = ['B2', 'B3', 'B4', 'B8', 'B8A', 'B11', 'B12']\n",
    "\n",
    "# Compute averages for each band\n",
    "for band in bands:\n",
    "    band_cols = [col for col in merged_df.columns if f'_{band}' in col]\n",
    "    merged_df[f'{band}_AVG'] = merged_df[band_cols].mean(axis=1)\n",
    "\n",
    "# Compute averages for cs and cs_CDF\n",
    "cs_cols = [col for col in merged_df.columns if '_cs' in col and '_cs_cdf' not in col]\n",
    "cs_cdf_cols = [col for col in merged_df.columns if '_cs_cdf' in col]\n",
    "\n",
    "merged_df['cs_AVG'] = merged_df[cs_cols].mean(axis=1)\n",
    "merged_df['cs_cdf_AVG'] = merged_df[cs_cdf_cols].mean(axis=1)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "updated_data_path = 'Updated_Merged_Data.csv'\n",
    "merged_df.to_csv(updated_data_path, index=False)\n",
    "\n",
    "print(\"Aggregation completed. The updated dataset is saved as 'Updated_Merged_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
